{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "VAE.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eewio8b7CBd2"
      },
      "source": [
        "Mostly based on https://github.com/aksub99/molecular-vae/blob/master/Molecular_VAE.ipynb \n",
        "\n",
        "Additions:\n",
        "\n",
        "- Property prediction segment and auxiliary loss\n",
        "- Different data prep (in load_data.ipynb) that more closely follows the original code https://github.com/aspuru-guzik-group/chemical_vae/\n",
        "- Sigmoid annealing schedule\n",
        "- Slower training it seems (TM)\n",
        "- Validation set and loss\n",
        "\n",
        "TODO:\n",
        "- better data loading (canonical only, less storage space)\n",
        "- teacher forcing gru"
      ],
      "id": "eewio8b7CBd2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "august-laundry",
        "outputId": "391851b3-9f2e-414c-b625-ce8877b0ceb5"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "import pandas as pd\n",
        "# imports the torch_xla package\n",
        "import os\n",
        "TPU = 'COLAB_TPU_ADDR' in os.environ\n",
        "if TPU:\n",
        "  !pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.7-cp36-cp36m-linux_x86_64.whl\n",
        "  import torch_xla\n",
        "  import torch_xla.core.xla_model as xm\n",
        "\n",
        "\n",
        "torch.manual_seed(42)"
      ],
      "id": "august-laundry",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f3b3f412b70>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLwZde_ZSNSf",
        "outputId": "c0a4438a-2d33-42fe-c356-07a99bf3f054"
      },
      "source": [
        "!git clone https://github.com/loodvn/pytorch-chemicalvae.git\n",
        "!mv pytorch-chemicalvae/data data\n",
        "# !ls data"
      ],
      "id": "FLwZde_ZSNSf",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'pytorch-chemicalvae'...\n",
            "remote: Enumerating objects: 30, done.\u001b[K\n",
            "remote: Counting objects: 100% (30/30), done.\u001b[K\n",
            "remote: Compressing objects: 100% (28/28), done.\u001b[K\n",
            "remote: Total 30 (delta 3), reused 22 (delta 1), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (30/30), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "accompanied-speaking"
      },
      "source": [
        "X = np.load('data/train_compressed.npz')['arr_0']\n",
        "Y = np.load('data/Y_reg.npy')\n",
        "# X = np.load('data/X_100.npy')\n",
        "# Y = np.load('data/Y_reg100.npy')"
      ],
      "id": "accompanied-speaking",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "therapeutic-fairy"
      },
      "source": [
        "# Put in load_data\n",
        "from torch.utils.data import DataLoader, TensorDataset, DataLoader\n",
        "\n",
        "TMP_TRAIN_SIZE = -1\n",
        "BATCH_SIZE = 256\n",
        "\n",
        "if TMP_TRAIN_SIZE < 0:\n",
        "    TMP_TRAIN_SIZE = Y.shape[0]\n",
        "# 75/25 split\n",
        "valid_idx = int(TMP_TRAIN_SIZE*0.75)\n",
        "x_train, y_train, x_valid, y_valid = map(torch.tensor, (X[:valid_idx], Y[:valid_idx], X[valid_idx:], Y[valid_idx:]))\n",
        "  \n",
        "del(X)  # Takes up too much RAM\n",
        "\n",
        "train_ds = TensorDataset(x_train, y_train)\n",
        "valid_ds = TensorDataset(x_valid, y_valid)\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "valid_loader = DataLoader(valid_ds, batch_size=2*BATCH_SIZE)"
      ],
      "id": "therapeutic-fairy",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "least-proceeding"
      },
      "source": [
        "class ChemVAE(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ChemVAE, self).__init__()\n",
        "        \n",
        "        self.latent_dims = 196  # p7, VAEs\n",
        "        self.num_char = 35 # including +1 for padding\n",
        "        \n",
        "        # From Methods/Autoencoder architecture section (p13)\n",
        "        self.enc_cnn1 = nn.Conv1d(in_channels=120, out_channels=9, kernel_size=9)  # 9,9\n",
        "        self.enc_cnn2 = nn.Conv1d(in_channels=9, out_channels=9, kernel_size=9)  # 9,9\n",
        "        self.enc_cnn3 = nn.Conv1d(in_channels=9, out_channels=11, kernel_size=10)  # 10, 11 (filter size, convolutional kernels)\n",
        "        \n",
        "        \n",
        "        self.enc_fc_mu = nn.Linear(11*10, self.latent_dims)  # 11  (out_channels * whatever's left?)\n",
        "        self.enc_fc_var = nn.Linear(11*10, self.latent_dims)  # 11\n",
        "        \n",
        "        \n",
        "        self.dec_gru = nn.GRU(input_size=self.latent_dims, hidden_size=488, num_layers=3, batch_first=True)  # TODO input_size is latent space?\n",
        "#         self.dec_gru_last = nn.GRU(input_size = self.latent_dims, hidden_size=488, )  # output GRU layer had one additional input, corresponding to the character sampled from the softmax output\n",
        "        self.dec_fc  = nn.Linear(488, self.num_char)\n",
        "        \n",
        "        self.property_1 = nn.Linear(self.latent_dims, 1000)\n",
        "        self.property_2 = nn.Linear(1000, 3)\n",
        "        self.property_dropout = nn.Dropout(p=0.2)\n",
        "        \n",
        "        # TODO activation functions? Assuming tanh not relu? Also, difference between F.relu and nn.ReLU?\n",
        "        self.act = F.relu\n",
        "        \n",
        "        \n",
        "    def encode(self, x):\n",
        "#         print(\"initial size:\", x.shape)\n",
        "        x = self.act(self.enc_cnn1(x))\n",
        "#         print(\"initial size:\", x.shape)\n",
        "        x = self.act(self.enc_cnn2(x))\n",
        "        x = self.act(self.enc_cnn3(x))\n",
        "#         print(\"size after enc_cnns:\", x.shape)\n",
        "\n",
        "        x = x.view(x.size(0), -1) # Flatten, Keep batch size\n",
        "        mu = self.enc_fc_mu(x)\n",
        "        var = self.enc_fc_var(x)\n",
        "\n",
        "        return mu, var\n",
        "\n",
        "    def decode(self, z):\n",
        "#         print(\"size before reshape\", z.size)\n",
        "        z = z.view(z.size(0), 1, z.size(-1))  # Expand_dims (1, latent_dim) -> (1, 1, latent_dim)\n",
        "#         print(\"size mid-reshape\", z.size)\n",
        "        z = z.repeat(1, 120, 1)               # Repeat latent*120: (1, 1, latent_dim) -> (1, 120, latent_dim)\n",
        "#         print(\"size after reshape\", z.size)\n",
        "        output, hn = self.dec_gru(z)\n",
        "        softmax = self.dec_fc(output)\n",
        "        softmax = F.softmax(softmax, dim=1)\n",
        "#         print(\"softmax shape:\", softmax.size())\n",
        "        return softmax\n",
        "        \n",
        "    \n",
        "    \n",
        "    # Copied from PyTorch VAE example\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5*logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps*std\n",
        "    \n",
        "    def prediction(self, z):\n",
        "        # two fully connected layers of 1000 neurons, dropout rate of 0.2\n",
        "        fc1 = self.act(self.property_dropout(self.property_1(z)))\n",
        "#         print(\"prop1 shape: \", fc1.shape)\n",
        "        pred = self.act(self.property_dropout(self.property_2(fc1)))\n",
        "#         print(\"prop 2 shape\", pred.shape)\n",
        "        \n",
        "        # output: batch size * 3\n",
        "        return pred\n",
        "    \n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar, z"
      ],
      "id": "least-proceeding",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "challenging-delta"
      },
      "source": [
        "Training\n",
        "- variational loss (KL divergence) annealed according to sigmoid schedule after 29 epochs, running for a total 120 epochs.\n",
        "- output GRU layer had one additional input, corresponding to the character sampled from the softmax output, trained using teacher forcing\n",
        "\n",
        "Getting output samples from softmax (depending on temperature):\n",
        "https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html#preparing-for-training\n",
        "\n",
        "Pytorch training loop over batches:\n",
        "loss.backward()\n",
        "opt.step()\n",
        "opt.zero_grad()\n",
        "\n",
        "Which reconstruction loss?\n",
        "CE loss?"
      ],
      "id": "challenging-delta"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hispanic-colombia"
      },
      "source": [
        "def one_hot_array(i, n):\n",
        "    return map(int, [ix == i for ix in xrange(n)])\n",
        "\n",
        "def one_hot_index(vec, charset):\n",
        "    return map(charset.index, vec)\n",
        "\n",
        "def from_one_hot_array(vec):\n",
        "    oh = np.where(vec == 1)\n",
        "    if oh[0].shape == (0, ):\n",
        "        return None\n",
        "    return int(oh[0][0])\n",
        "\n",
        "def decode_smiles_from_indexes(vec, charset):\n",
        "    return \"\".join(map(lambda x: charset[x], vec)).strip()\n",
        "\n",
        "charset = ['n',\n",
        " '[',\n",
        " 'o',\n",
        " 'I',\n",
        " '3',\n",
        " 'H',\n",
        " '+',\n",
        " 'S',\n",
        " '@',\n",
        " '8',\n",
        " '4',\n",
        " '1',\n",
        " 's',\n",
        " 'N',\n",
        " 'F',\n",
        " 'P',\n",
        " '/',\n",
        " '=',\n",
        " 'O',\n",
        " 'B',\n",
        " 'C',\n",
        " '\\\\',\n",
        " '(',\n",
        " '-',\n",
        " ']',\n",
        " '6',\n",
        " ')',\n",
        " 'r',\n",
        " '5',\n",
        " '7',\n",
        " '2',\n",
        " '#',\n",
        " 'l',\n",
        " 'c',\n",
        " ' ']"
      ],
      "id": "hispanic-colombia",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acoustic-surgery",
        "outputId": "dd65f5b7-f8cf-4c6f-9f41-65939948e582"
      },
      "source": [
        "def sigmoid_schedule(time_step, slope=1., start=22):\n",
        "    return float(1 / (1. + np.exp(slope * (start - float(time_step)))))\n",
        "sigmoid_schedule()"
      ],
      "id": "acoustic-surgery",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9996646498695336"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "express-stage"
      },
      "source": [
        "# Baseline: Mean prediction\n"
      ],
      "id": "express-stage"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "actual-framework",
        "outputId": "f7264ccf-f201-4670-ec77-ef30a36a581b"
      },
      "source": [
        "# \n",
        "logP = np.mean(np.abs(Y[:,0].mean()-Y[:,0]))\n",
        "print(\"logP baseline: \", logP)\n",
        "QED = np.mean(np.abs(Y[:,1].mean()-Y[:,1]))\n",
        "print(\"QED baseline: \", QED)"
      ],
      "id": "actual-framework",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "logP baseline:  1.1381030935900205\n",
            "QED baseline:  0.1121743723222245\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vanilla-detail",
        "outputId": "db878950-bdfc-4318-94e2-6a8d42108434"
      },
      "source": [
        "(np.abs(Y.mean(axis=0)-Y)).mean(axis=0)  # logP, QED, SAS"
      ],
      "id": "vanilla-detail",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.13810309, 0.11217437, 0.66557906])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "convenient-virus"
      },
      "source": [
        "## Train"
      ],
      "id": "convenient-virus"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyW0eyCW410v"
      },
      "source": [
        "# From other pytorch implementation\n",
        "def vae_loss(x_decoded_mean, x, z_mean, z_logvar):\n",
        "    xent_loss = F.binary_cross_entropy(x_decoded_mean, x, reduction='sum')\n",
        "    kl_loss = -0.5 * torch.sum(1 + z_logvar - z_mean.pow(2) - z_logvar.exp())\n",
        "    return xent_loss + kl_loss\n",
        "\n",
        "def xent_loss(x_decoded_mean, x):\n",
        "    return F.binary_cross_entropy(x_decoded_mean, x, reduction='sum')\n",
        "\n",
        "def kl_loss(z_mean, z_logvar):\n",
        "    return -0.5 * torch.sum(1 + z_logvar - z_mean.pow(2) - z_logvar.exp())\n",
        "\n",
        "# prediction loss: mse\n",
        "def pred_loss(y_pred, y_true):\n",
        "    return torch.mean((y_pred - y_true).pow(2)).to(device)\n",
        "\n",
        "def mae(y_pred, y_true):\n",
        "    return torch.mean(torch.abs(y_pred - y_true))"
      ],
      "id": "CyW0eyCW410v",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spectacular-boston",
        "outputId": "13427609-63a5-4225-caf2-9a627bc5f59a"
      },
      "source": [
        "print(\"Starting training\")\n",
        "import time\n",
        "start = time.time()\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# device = xm.xla_device()\n",
        "epochs = 20 #120\n",
        "\n",
        "model = ChemVAE().to(device)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "SIGMOID_ANNEALING = True\n",
        "\n",
        "# From other pytorch implementation TODO reference properly\n",
        "# TODO save checkpoints every 1 hours\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, data in enumerate(train_loader):\n",
        "        y_true = data[1].to(device)\n",
        "        data = data[0].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output, mean, logvar, z = model(data)\n",
        "        pred = model.prediction(z)\n",
        "#         print(\"pred:\", pred.shape, \"y: \", y_true.shape)\n",
        "        \n",
        "        if batch_idx==0:\n",
        "              inp = data.cpu().numpy()\n",
        "              outp = output.cpu().detach().numpy()\n",
        "              lab = data.cpu().numpy()\n",
        "              print(\"Input:\")\n",
        "              print(decode_smiles_from_indexes(map(from_one_hot_array, inp[0]), charset))\n",
        "              print(\"Label:\")\n",
        "              print(decode_smiles_from_indexes(map(from_one_hot_array, lab[0]), charset))\n",
        "              sampled = outp[0].reshape(1, 120, len(charset)).argmax(axis=2)[0]\n",
        "              print(\"Output:\")\n",
        "              print(decode_smiles_from_indexes(sampled, charset))\n",
        "\n",
        "        \n",
        "#         print(\"pred loss: \", pred_loss(pred, y_true), \"shape: \", pred_loss(pred, y_true).shape)\n",
        "        sched = torch.tensor(sigmoid_schedule(epoch)).to(device) if SIGMOID_ANNEALING else 1\n",
        "        loss = sched*kl_loss(mean, logvar) + xent_loss(output, data) + sched*pred_loss(pred, y_true)\n",
        "        # import pdb; pdb.set_trace()\n",
        "        loss.backward()\n",
        "        train_loss += loss\n",
        "        optimizer.step()\n",
        "        if TPU:\n",
        "          xm.mark_step()\n",
        "        \n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f'epoch {epoch} / batch {batch_idx}\\tFull loss: {loss/BATCH_SIZE:.4f}')  # TODO print all of the loss components seperately\n",
        "            pred_mae = mae(pred, y_true)\n",
        "            print(f'epoch {epoch} / batch {batch_idx}\\tPred mae loss: {pred_mae/BATCH_SIZE:.4f}')\n",
        "#     print(f'epoch {epoch}: train loss:', (train_loss / len(train_loader.dataset)))\n",
        "    return train_loss / len(train_loader.dataset)\n",
        "\n",
        "def eval_model():\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      eval_loss = 0\n",
        "      eval_pred_loss = 0\n",
        "      logP_loss = 0\n",
        "      QED_loss = 0\n",
        "\n",
        "      for batch_idx, data in enumerate(valid_loader):\n",
        "        y_true = data[1].to(device)\n",
        "        data = data[0].to(device)\n",
        "        output, mean, logvar, z = model(data)\n",
        "        pred = model.prediction(z)\n",
        "\n",
        "        sched = torch.tensor(sigmoid_schedule(epoch)).to(device)\n",
        "        loss = sched*kl_loss(mean, logvar) + xent_loss(output, data) + sched*pred_loss(pred, y_true)\n",
        "        \n",
        "        eval_loss += loss\n",
        "        eval_pred_loss += pred_loss(pred, y_true)\n",
        "        logP_loss += torch.sum(torch.abs(pred[:,0] - y_true[:,0]))  # MAE loss to reproduce Table 2\n",
        "        QED_loss += torch.sum(torch.abs(pred[:,1] - y_true[:,1]))  # MAE loss to reproduce Table 2\n",
        "\n",
        "    return eval_loss / len(valid_loader.dataset), eval_pred_loss / len(valid_loader.dataset), torch.sum(logP_loss) / len(valid_loader.dataset), torch.sum(QED_loss / len(valid_loader.dataset))\n",
        "\n",
        "val_losses = []\n",
        "val_pred_losses = []\n",
        "logP_losses = []\n",
        "qed_losses = []\n",
        "for epoch in range(1, epochs + 1):\n",
        "    e_start = time.time()\n",
        "    train_loss = train(epoch)\n",
        "    print(f\"{epoch} Training loss: {train_loss}\")\n",
        "    e_end = time.time()\n",
        "    print(f\"Time per epoch ({epoch}): {e_end-e_start:.3f}s\")\n",
        "\n",
        "    print(\"Evaluating...\")\n",
        "    val_loss, eval_pred_loss, logP_loss, qed_loss = eval_model()\n",
        "    print(f\"Evaluation loss (training): \\n{val_loss}, \\n{eval_pred_loss}, \\n{logP_loss}, \\n{qed_loss}\")\n",
        "    \n",
        "    val_losses.append(val_loss.item())\n",
        "    val_pred_losses.append(eval_pred_loss.item())\n",
        "    logP_losses.append(logP_loss.item())\n",
        "    qed_losses.append(qed_loss.item())\n",
        "    print(f\"Elapsed time: {e_end-start:.3f}s\")\n",
        "\n",
        "end = time.time()\n",
        "print(f\"Total time taken: {int(end-start)//60}m{(end-start)%60:.3f}s\")"
      ],
      "id": "spectacular-boston",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting training\n",
            "Input:\n",
            "44-on6nF(FFFF-r4PP+\\c4-4-on6n4-4646o4-46Oo41#444-on6O1c6F(\n",
            "Label:\n",
            "44-on6nF(FFFF-r4PP+\\c4-4-on6n4-4646o4-46Oo41#444-on6O1c6F(\n",
            "Output:\n",
            "BB====ssssOOOOOrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr\n",
            "epoch 1 / batch 0\tFull loss: 608.6711\n",
            "epoch 1 / batch 0\tPred mae loss: 0.0078\n",
            "epoch 1 / batch 100\tFull loss: 534.9409\n",
            "epoch 1 / batch 100\tPred mae loss: 0.0066\n",
            "epoch 1 / batch 200\tFull loss: 530.1423\n",
            "epoch 1 / batch 200\tPred mae loss: 0.0047\n",
            "epoch 1 / batch 300\tFull loss: 527.1787\n",
            "epoch 1 / batch 300\tPred mae loss: 0.0049\n",
            "epoch 1 / batch 400\tFull loss: 529.0965\n",
            "epoch 1 / batch 400\tPred mae loss: 0.0050\n",
            "epoch 1 / batch 500\tFull loss: 527.3892\n",
            "epoch 1 / batch 500\tPred mae loss: 0.0049\n",
            "epoch 1 / batch 600\tFull loss: 526.2923\n",
            "epoch 1 / batch 600\tPred mae loss: 0.0048\n",
            "epoch 1 / batch 700\tFull loss: 526.9380\n",
            "epoch 1 / batch 700\tPred mae loss: 0.0052\n",
            "1 Training loss: 531.5980829516166\n",
            "Time per epoch (1): 224.476s\n",
            "Evaluating...\n",
            "Evaluation loss (training): \n",
            "525.7607714386525, \n",
            "0.0044481303137324365, \n",
            "1.5891182218766429, \n",
            "0.44853291078721746\n",
            "Elapsed time: 224.512s\n",
            "Input:\n",
            "44(oO4co4-4-on644-46-464c6r4PP+\\-FcFHF1FFF-476FF1Fcon64(4-on6n4r4P+\\(444n(\n",
            "Label:\n",
            "44(oO4co4-4-on644-46-464c6r4PP+\\-FcFHF1FFF-476FF1Fcon64(4-on6n4r4P+\\(444n(\n",
            "Output:\n",
            "4=((PFBBNNNNNN3333333333333331111112222222222222222222222227777776HH(((((\n",
            "epoch 2 / batch 0\tFull loss: 525.6615\n",
            "epoch 2 / batch 0\tPred mae loss: 0.0051\n",
            "epoch 2 / batch 100\tFull loss: 528.0204\n",
            "epoch 2 / batch 100\tPred mae loss: 0.0053\n",
            "epoch 2 / batch 200\tFull loss: 525.5440\n",
            "epoch 2 / batch 200\tPred mae loss: 0.0048\n",
            "epoch 2 / batch 300\tFull loss: 525.8612\n",
            "epoch 2 / batch 300\tPred mae loss: 0.0050\n",
            "epoch 2 / batch 400\tFull loss: 524.6840\n",
            "epoch 2 / batch 400\tPred mae loss: 0.0052\n",
            "epoch 2 / batch 500\tFull loss: 524.3312\n",
            "epoch 2 / batch 500\tPred mae loss: 0.0050\n",
            "epoch 2 / batch 600\tFull loss: 524.6392\n",
            "epoch 2 / batch 600\tPred mae loss: 0.0049\n",
            "epoch 2 / batch 700\tFull loss: 523.1827\n",
            "epoch 2 / batch 700\tPred mae loss: 0.0050\n",
            "2 Training loss: 525.5971898785403\n",
            "Time per epoch (2): 227.124s\n",
            "Evaluating...\n",
            "Evaluation loss (training): \n",
            "523.8516013345117, \n",
            "0.004239224657858304, \n",
            "1.545638545048721, \n",
            "0.43128183055934566\n",
            "Elapsed time: 473.036s\n",
            "Input:\n",
            "44F(FFF-n44-on6OFcFFF-n46F-n46Fc6FF(\n",
            "Label:\n",
            "44F(FFF-n44-on6OFcFFF-n46F-n46Fc6FF(\n",
            "Output:\n",
            "4=((FFBBBNNoOOOOOO+2222222211116c6H((ll\n",
            "epoch 3 / batch 0\tFull loss: 524.1772\n",
            "epoch 3 / batch 0\tPred mae loss: 0.0052\n",
            "epoch 3 / batch 100\tFull loss: 522.8989\n",
            "epoch 3 / batch 100\tPred mae loss: 0.0047\n",
            "epoch 3 / batch 200\tFull loss: 521.9429\n",
            "epoch 3 / batch 200\tPred mae loss: 0.0045\n",
            "epoch 3 / batch 300\tFull loss: 519.5008\n",
            "epoch 3 / batch 300\tPred mae loss: 0.0046\n",
            "epoch 3 / batch 400\tFull loss: 519.3312\n",
            "epoch 3 / batch 400\tPred mae loss: 0.0044\n",
            "epoch 3 / batch 500\tFull loss: 520.1910\n",
            "epoch 3 / batch 500\tPred mae loss: 0.0044\n",
            "epoch 3 / batch 600\tFull loss: 522.8214\n",
            "epoch 3 / batch 600\tPred mae loss: 0.0041\n",
            "epoch 3 / batch 700\tFull loss: 517.8519\n",
            "epoch 3 / batch 700\tPred mae loss: 0.0045\n",
            "3 Training loss: 520.9322349318567\n",
            "Time per epoch (3): 227.169s\n",
            "Evaluating...\n",
            "Evaluation loss (training): \n",
            "517.2938484141664, \n",
            "0.003162409713803448, \n",
            "1.3460310502963573, \n",
            "0.29288188071276244\n",
            "Elapsed time: 721.703s\n",
            "Input:\n",
            "4n444O-464-on6O44(-FcFFFF-476Fc644n44(\n",
            "Label:\n",
            "4n444O-464-on6O44(-FcFFFF-476Fc644n44(\n",
            "Output:\n",
            "no4-O#BPPP\\\\]]]]]]]BBBB2211111117c6(((l\n",
            "epoch 4 / batch 0\tFull loss: 517.6566\n",
            "epoch 4 / batch 0\tPred mae loss: 0.0043\n",
            "epoch 4 / batch 100\tFull loss: 517.3341\n",
            "epoch 4 / batch 100\tPred mae loss: 0.0042\n",
            "epoch 4 / batch 200\tFull loss: 520.4430\n",
            "epoch 4 / batch 200\tPred mae loss: 0.0037\n",
            "epoch 4 / batch 300\tFull loss: 515.3638\n",
            "epoch 4 / batch 300\tPred mae loss: 0.0038\n",
            "epoch 4 / batch 400\tFull loss: 513.3717\n",
            "epoch 4 / batch 400\tPred mae loss: 0.0036\n",
            "epoch 4 / batch 500\tFull loss: 516.4756\n",
            "epoch 4 / batch 500\tPred mae loss: 0.0037\n",
            "epoch 4 / batch 600\tFull loss: 513.6623\n",
            "epoch 4 / batch 600\tPred mae loss: 0.0037\n",
            "epoch 4 / batch 700\tFull loss: 517.4427\n",
            "epoch 4 / batch 700\tPred mae loss: 0.0039\n",
            "4 Training loss: 516.375097755026\n",
            "Time per epoch (4): 227.304s\n",
            "Evaluating...\n",
            "Evaluation loss (training): \n",
            "513.5355508587027, \n",
            "0.002233333833994014, \n",
            "1.2038127357173407, \n",
            "0.19711312677265788\n",
            "Elapsed time: 970.566s\n",
            "Input:\n",
            "4r4P+\\(44r4P+\\-4-O6on64O(4-on6F(FFF-Sl6FF(I\n",
            "Label:\n",
            "4r4P+\\(44r4P+\\-4-O6on64O(4-on6F(FFF-Sl6FF(I\n",
            "Output:\n",
            "4r4P]\\\\-O4ooooOOOooooo11111BBBBBBFFISSl[[[(l\n",
            "epoch 5 / batch 0\tFull loss: 513.7439\n",
            "epoch 5 / batch 0\tPred mae loss: 0.0036\n",
            "epoch 5 / batch 100\tFull loss: 513.6252\n",
            "epoch 5 / batch 100\tPred mae loss: 0.0037\n",
            "epoch 5 / batch 200\tFull loss: 513.8818\n",
            "epoch 5 / batch 200\tPred mae loss: 0.0036\n",
            "epoch 5 / batch 300\tFull loss: 514.4072\n",
            "epoch 5 / batch 300\tPred mae loss: 0.0035\n",
            "epoch 5 / batch 400\tFull loss: 510.9712\n",
            "epoch 5 / batch 400\tPred mae loss: 0.0035\n",
            "epoch 5 / batch 500\tFull loss: 513.0405\n",
            "epoch 5 / batch 500\tPred mae loss: 0.0037\n",
            "epoch 5 / batch 600\tFull loss: 510.0500\n",
            "epoch 5 / batch 600\tPred mae loss: 0.0034\n",
            "epoch 5 / batch 700\tFull loss: 510.7104\n",
            "epoch 5 / batch 700\tPred mae loss: 0.0033\n",
            "5 Training loss: 512.1098198039113\n",
            "Time per epoch (5): 227.029s\n",
            "Evaluating...\n",
            "Evaluation loss (training): \n",
            "509.5366783120901, \n",
            "0.0020096337241573945, \n",
            "1.1882883701697722, \n",
            "0.172120624996648\n",
            "Elapsed time: 1219.094s\n",
            "Input:\n",
            "444rO+]\\-44-on6O4(44n44(6r4P+\\-46F(FFcFFFFFcH(\n",
            "Label:\n",
            "444rO+]\\-44-on6O4(44n44(6r4P+\\-46F(FFcFFFFFcH(\n",
            "Output:\n",
            "4=OrOP+\\\\rr++\\\\\\\\]]]]rIIIIIII7BBBBBBBcFFFFFFH(l2\n",
            "epoch 6 / batch 0\tFull loss: 509.1950\n",
            "epoch 6 / batch 0\tPred mae loss: 0.0036\n",
            "epoch 6 / batch 100\tFull loss: 508.6133\n",
            "epoch 6 / batch 100\tPred mae loss: 0.0036\n",
            "epoch 6 / batch 200\tFull loss: 509.2759\n",
            "epoch 6 / batch 200\tPred mae loss: 0.0034\n",
            "epoch 6 / batch 300\tFull loss: 508.2637\n",
            "epoch 6 / batch 300\tPred mae loss: 0.0034\n",
            "epoch 6 / batch 400\tFull loss: 506.6236\n",
            "epoch 6 / batch 400\tPred mae loss: 0.0031\n",
            "epoch 6 / batch 500\tFull loss: 505.2711\n",
            "epoch 6 / batch 500\tPred mae loss: 0.0035\n",
            "epoch 6 / batch 600\tFull loss: 505.8474\n",
            "epoch 6 / batch 600\tPred mae loss: 0.0035\n",
            "epoch 6 / batch 700\tFull loss: 506.2203\n",
            "epoch 6 / batch 700\tPred mae loss: 0.0031\n",
            "6 Training loss: 507.4310587837011\n",
            "Time per epoch (6): 227.431s\n",
            "Evaluating...\n",
            "Evaluation loss (training): \n",
            "504.7155928803027, \n",
            "0.0017374305660640993, \n",
            "1.099517301896774, \n",
            "0.18044195989150283\n",
            "Elapsed time: 1468.132s\n",
            "Input:\n",
            "4O-Or4P+\\(n4-on6FcFFFFFc(6F(@FF-4-I6-I6I6FF(47\n",
            "Label:\n",
            "4O-Or4P+\\(n4-on6FcFFFFFc(6F(@FF-4-I6-I6I6FF(47\n",
            "Output:\n",
            "4O#46rPP+\\\\OOONNNFFFFFFS771111777IIIrr==Pc+(((l\n",
            "epoch 7 / batch 0\tFull loss: 502.8711\n",
            "epoch 7 / batch 0\tPred mae loss: 0.0032\n",
            "epoch 7 / batch 100\tFull loss: 503.5857\n",
            "epoch 7 / batch 100\tPred mae loss: 0.0032\n",
            "epoch 7 / batch 200\tFull loss: 502.7783\n",
            "epoch 7 / batch 200\tPred mae loss: 0.0034\n",
            "epoch 7 / batch 300\tFull loss: 501.9120\n",
            "epoch 7 / batch 300\tPred mae loss: 0.0033\n",
            "epoch 7 / batch 400\tFull loss: 506.1552\n",
            "epoch 7 / batch 400\tPred mae loss: 0.0033\n",
            "epoch 7 / batch 500\tFull loss: 502.1955\n",
            "epoch 7 / batch 500\tPred mae loss: 0.0033\n",
            "epoch 7 / batch 600\tFull loss: 500.7071\n",
            "epoch 7 / batch 600\tPred mae loss: 0.0033\n",
            "epoch 7 / batch 700\tFull loss: 501.3077\n",
            "epoch 7 / batch 700\tPred mae loss: 0.0030\n",
            "7 Training loss: 502.13028682607154\n",
            "Time per epoch (7): 226.626s\n",
            "Evaluating...\n",
            "Evaluation loss (training): \n",
            "499.11287561784667, \n",
            "0.0016457425313479645, \n",
            "1.0753337272595478, \n",
            "0.17617613955189848\n",
            "Elapsed time: 1716.333s\n",
            "Input:\n",
            "4F(FFFF-BFcHF@Fc4-on6O4c44c6F(\n",
            "Label:\n",
            "4F(FFFF-BFcHF@Fc4-on6O4c44c6F(\n",
            "Output:\n",
            "4F(FFFFBBcc@@@77oooPPPP]\\\\c66((l\n",
            "epoch 8 / batch 0\tFull loss: 496.4005\n",
            "epoch 8 / batch 0\tPred mae loss: 0.0030\n",
            "epoch 8 / batch 100\tFull loss: 503.9096\n",
            "epoch 8 / batch 100\tPred mae loss: 0.0034\n",
            "epoch 8 / batch 200\tFull loss: 497.8849\n",
            "epoch 8 / batch 200\tPred mae loss: 0.0032\n",
            "epoch 8 / batch 300\tFull loss: 498.7726\n",
            "epoch 8 / batch 300\tPred mae loss: 0.0030\n",
            "epoch 8 / batch 400\tFull loss: 498.7856\n",
            "epoch 8 / batch 400\tPred mae loss: 0.0031\n",
            "epoch 8 / batch 500\tFull loss: 497.0222\n",
            "epoch 8 / batch 500\tPred mae loss: 0.0031\n",
            "epoch 8 / batch 600\tFull loss: 495.0138\n",
            "epoch 8 / batch 600\tPred mae loss: 0.0030\n",
            "epoch 8 / batch 700\tFull loss: 495.6552\n",
            "epoch 8 / batch 700\tPred mae loss: 0.0031\n",
            "8 Training loss: 497.70179522976923\n",
            "Time per epoch (8): 228.475s\n",
            "Evaluating...\n",
            "Evaluation loss (training): \n",
            "495.8523870453506, \n",
            "0.0013740549215137232, \n",
            "1.000579249611853, \n",
            "0.15877659588149043\n",
            "Elapsed time: 1966.416s\n",
            "Input:\n",
            "44r4P+\\-46r4P+\\-rO+1]\\64-on6O(44r4PP+\\-4-on6rnB\\6r4PP+\\(4\n",
            "Label:\n",
            "44r4P+\\-46r4P+\\-rO+1]\\64-on6O(44r4PP+\\-4-on6rnB\\6r4PP+\\(4\n",
            "Output:\n",
            "4NrOP]\\-46rrP+\\-NNNNN333377777II2222222222HHHHoooooBB@+@((((\n",
            "epoch 9 / batch 0\tFull loss: 495.0569\n",
            "epoch 9 / batch 0\tPred mae loss: 0.0031\n",
            "epoch 9 / batch 100\tFull loss: 493.5989\n",
            "epoch 9 / batch 100\tPred mae loss: 0.0029\n",
            "epoch 9 / batch 200\tFull loss: 493.1022\n",
            "epoch 9 / batch 200\tPred mae loss: 0.0028\n",
            "epoch 9 / batch 300\tFull loss: 491.5138\n",
            "epoch 9 / batch 300\tPred mae loss: 0.0030\n",
            "epoch 9 / batch 400\tFull loss: 493.5398\n",
            "epoch 9 / batch 400\tPred mae loss: 0.0030\n",
            "epoch 9 / batch 500\tFull loss: 493.7528\n",
            "epoch 9 / batch 500\tPred mae loss: 0.0028\n",
            "epoch 9 / batch 600\tFull loss: 487.5118\n",
            "epoch 9 / batch 600\tPred mae loss: 0.0031\n",
            "epoch 9 / batch 700\tFull loss: 489.4045\n",
            "epoch 9 / batch 700\tPred mae loss: 0.0029\n",
            "9 Training loss: 492.8193542158845\n",
            "Time per epoch (9): 227.282s\n",
            "Evaluating...\n",
            "Evaluation loss (training): \n",
            "488.9634855476409, \n",
            "0.0013696493213479195, \n",
            "0.956428049441601, \n",
            "0.16009621422738493\n",
            "Elapsed time: 2215.338s\n",
            "Input:\n",
            "4o44@(NF-oON4-on64-4646[FcFFFF-46Fc(\n",
            "Label:\n",
            "4o44@(NF-oON4-on64-4646[FcFFFF-46Fc(\n",
            "Output:\n",
            "4=4P@(@H-OOOOroon6rPP\\\\11c]II77776F((l/5\n",
            "epoch 10 / batch 0\tFull loss: 489.9165\n",
            "epoch 10 / batch 0\tPred mae loss: 0.0030\n",
            "epoch 10 / batch 100\tFull loss: 489.4626\n",
            "epoch 10 / batch 100\tPred mae loss: 0.0032\n",
            "epoch 10 / batch 200\tFull loss: 493.2443\n",
            "epoch 10 / batch 200\tPred mae loss: 0.0029\n",
            "epoch 10 / batch 300\tFull loss: 487.1396\n",
            "epoch 10 / batch 300\tPred mae loss: 0.0029\n",
            "epoch 10 / batch 400\tFull loss: 486.6743\n",
            "epoch 10 / batch 400\tPred mae loss: 0.0030\n",
            "epoch 10 / batch 500\tFull loss: 484.9456\n",
            "epoch 10 / batch 500\tPred mae loss: 0.0029\n",
            "epoch 10 / batch 600\tFull loss: 486.1414\n",
            "epoch 10 / batch 600\tPred mae loss: 0.0031\n",
            "epoch 10 / batch 700\tFull loss: 484.8124\n",
            "epoch 10 / batch 700\tPred mae loss: 0.0030\n",
            "10 Training loss: 487.79136698237875\n",
            "Time per epoch (10): 227.267s\n",
            "Evaluating...\n",
            "Evaluation loss (training): \n",
            "485.74469195748, \n",
            "0.0012690981464789817, \n",
            "0.9119815838455575, \n",
            "0.16843919009380456\n",
            "Elapsed time: 2464.243s\n",
            "Input:\n",
            "4r4PP+\\(44r4P\\-n6-r4P\\c-4rO+1]\\644F1FFFFF1c64(\n",
            "Label:\n",
            "4r4PP+\\(44r4P\\-n6-r4P\\c-4rO+1]\\644F1FFFFF1c64(\n",
            "Output:\n",
            "4r4PP+\\(nrrPP+--OOO]]]\\NPP+\\oonnB11FFFFF1[c66((7n\n",
            "epoch 11 / batch 0\tFull loss: 485.5000\n",
            "epoch 11 / batch 0\tPred mae loss: 0.0029\n",
            "epoch 11 / batch 100\tFull loss: 484.5204\n",
            "epoch 11 / batch 100\tPred mae loss: 0.0029\n",
            "epoch 11 / batch 200\tFull loss: 483.5239\n",
            "epoch 11 / batch 200\tPred mae loss: 0.0029\n",
            "epoch 11 / batch 300\tFull loss: 481.7700\n",
            "epoch 11 / batch 300\tPred mae loss: 0.0031\n",
            "epoch 11 / batch 400\tFull loss: 485.3153\n",
            "epoch 11 / batch 400\tPred mae loss: 0.0032\n",
            "epoch 11 / batch 500\tFull loss: 480.2419\n",
            "epoch 11 / batch 500\tPred mae loss: 0.0029\n",
            "epoch 11 / batch 600\tFull loss: 480.9033\n",
            "epoch 11 / batch 600\tPred mae loss: 0.0028\n",
            "epoch 11 / batch 700\tFull loss: 480.2099\n",
            "epoch 11 / batch 700\tPred mae loss: 0.0030\n",
            "11 Training loss: 482.8887476754276\n",
            "Time per epoch (11): 226.284s\n",
            "Evaluating...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwpOzsJ2AEV0"
      },
      "source": [
        "Plot losses"
      ],
      "id": "RwpOzsJ2AEV0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oKueqKb5m4g"
      },
      "source": [
        "df = pd.DataFrame({\"val\": val_losses, \"val_pred\": val_pred_losses, \"logP\": logP_losses, \"qed\": qed_losses})\n",
        "df_total = df['val']\n",
        "df_pred = df.drop(columns=[\"val\"])\n",
        "df_pred.plot()"
      ],
      "id": "3oKueqKb5m4g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gFxiASieFex"
      },
      "source": [
        "## tmp\n",
        "TPU Error: /usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\n",
        "    130     retain_graph: Optional[bool] = None,\n",
        "    131     create_graph: bool = False,\n",
        "--> 132     only_inputs: bool = True,\n",
        "    133     allow_unused: bool = False\n",
        "    134 ) -> Tuple[torch.Tensor, ...]:\n",
        "\n",
        "RuntimeError: vector::_M_range_check: __n (which is 1) >= this->size() (which is 1)\n",
        "\n",
        "Loss after ~10 mins:\n",
        "Evaluation loss (training):  (tensor(513.6445, device='cuda:0', dtype=torch.float64), tensor(0.0252, device='cuda:0', dtype=torch.float64), tensor([0.0208, 0.0050, 0.0183], device='cuda:0', dtype=torch.float64), tensor([0.0208, 0.0050, 0.0183], device='cuda:0', dtype=torch.float64\n"
      ],
      "id": "7gFxiASieFex"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeDpYJmEJIym"
      },
      "source": [
        "Starting training\n",
        "Input:\n",
        "4@(FF-BFcFFFF-4rO+]\\144r4PP\\-n6-4-I6-I6I6416Fc6F@(\n",
        "Label:\n",
        "4@(FF-BFcFFFF-4rO+]\\144r4PP\\-n6-4-I6-I6I6416Fc6F@(\n",
        "Output:\n",
        "ssssssssssn#111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111\n",
        "epoch 1 / batch 0\tFull loss: 155817.0156\n",
        "epoch 1 / batch 0\tPred loss: 1.9640\n",
        "epoch 1 / batch 100\tFull loss: 135395.6875\n",
        "epoch 1 / batch 100\tPred loss: 1.2291\n",
        "epoch 1 / batch 200\tFull loss: 135623.5313\n",
        "epoch 1 / batch 200\tPred loss: 1.1131\n",
        "epoch 1 / batch 300\tFull loss: 135002.1563\n",
        "epoch 1 / batch 300\tPred loss: 1.1840\n",
        "epoch 1 / batch 400\tFull loss: 135334.3438\n",
        "epoch 1 / batch 400\tPred loss: 1.2385\n",
        "epoch 1 / batch 500\tFull loss: 134454.6094\n",
        "epoch 1 / batch 500\tPred loss: 1.2087\n",
        "epoch 1 / batch 600\tFull loss: 134295.0781\n",
        "epoch 1 / batch 600\tPred loss: 1.2694\n",
        "epoch 1 / batch 700\tFull loss: 134368.5938\n",
        "epoch 1 / batch 700\tPred loss: 1.2350\n",
        "Time per epoch (1): 208.468s\n",
        "Evaluating...\n",
        "Evaluation loss (training): \n",
        "523.524375040089, \n",
        "0.004267985728560455, \n",
        "1.5497679990182962, \n",
        "0.3987843898209971\n",
        "Elapsed time: 218.533s\n",
        "Input:\n",
        "44O(44rO+]\\c444Or4P+\\c4(on\n",
        "Label:\n",
        "44O(44rO+]\\c444Or4P+\\c4(on\n",
        "Output:\n",
        "4=(1PFBBNNNNN222221666H(((l\n",
        "epoch 2 / batch 0\tFull loss: 134005.5000\n",
        "epoch 2 / batch 0\tPred loss: 1.2578\n",
        "epoch 2 / batch 100\tFull loss: 133798.4688\n",
        "epoch 2 / batch 100\tPred loss: 1.2414\n",
        "epoch 2 / batch 200\tFull loss: 134115.9375\n",
        "epoch 2 / batch 200\tPred loss: 1.3186\n",
        "epoch 2 / batch 300\tFull loss: 134341.1875\n",
        "epoch 2 / batch 300\tPred loss: 1.2813\n",
        "epoch 2 / batch 400\tFull loss: 133899.8438\n",
        "epoch 2 / batch 400\tPred loss: 1.2360\n",
        "epoch 2 / batch 500\tFull loss: 133523.3281\n",
        "epoch 2 / batch 500\tPred loss: 1.2713\n",
        "epoch 2 / batch 600\tFull loss: 134387.4063\n",
        "epoch 2 / batch 600\tPred loss: 1.2330\n",
        "epoch 2 / batch 700\tFull loss: 133031.4063\n",
        "epoch 2 / batch 700\tPred loss: 1.2627\n",
        "Time per epoch (2): 223.383s\n",
        "Evaluating...\n",
        "Evaluation loss (training): \n",
        "518.1136467954343, \n",
        "0.003951255076182853, \n",
        "1.5243500593229338, \n",
        "0.36010640248726383\n",
        "Elapsed time: 462.739s\n",
        "Input:\n",
        "44r4PP+\\-4F(FFFFF(6O4-on6O(444r4P+\\-4-on6Oc44n44c64(\n",
        "Label:\n",
        "44r4PP+\\-4F(FFFFF(6O4-on6O(444r4P+\\-4-on6Oc44n44c64(\n",
        "Output:\n",
        "no#-]]\\((@FFFFFFFFNNNNNO332222222222222221111cc6H(((\n",
        "epoch 3 / batch 0\tFull loss: 132805.6719\n",
        "epoch 3 / batch 0\tPred loss: 1.2380\n",
        "epoch 3 / batch 100\tFull loss: 133457.9688\n",
        "epoch 3 / batch 100\tPred loss: 1.1281\n",
        "epoch 3 / batch 200\tFull loss: 132526.6563\n",
        "epoch 3 / batch 200\tPred loss: 1.0878\n",
        "epoch 3 / batch 300\tFull loss: 131957.4688\n",
        "epoch 3 / batch 300\tPred loss: 1.0116\n",
        "epoch 3 / batch 400\tFull loss: 132188.6094\n",
        "epoch 3 / batch 400\tPred loss: 1.0222\n",
        "epoch 3 / batch 500\tFull loss: 131492.4531\n",
        "epoch 3 / batch 500\tPred loss: 0.9942\n",
        "epoch 3 / batch 600\tFull loss: 131535.2813\n",
        "epoch 3 / batch 600\tPred loss: 0.9581\n",
        "epoch 3 / batch 700\tFull loss: 130860.2813\n",
        "epoch 3 / batch 700\tPred loss: 1.0071\n",
        "Time per epoch (3): 225.783s\n",
        "Evaluating...\n",
        "Evaluation loss (training): \n",
        "511.09088476526335, \n",
        "0.002368703126348485, \n",
        "1.230272634271861, \n",
        "0.21858335207106477\n",
        "Elapsed time: 710.049s\n",
        "Input:\n",
        "44-46-46@(FF-4-on6O44c-4-O6on644n44c6F@(\n",
        "Label:\n",
        "44-46-46@(FF-4-on6O44c-4-O6on644n44c6F@(\n",
        "Output:\n",
        "4=--6nnF(FFFrrrPrr]]++\\\\\\\\\\\\111111cccH((l\n",
        "epoch 4 / batch 0\tFull loss: 130977.4609\n",
        "epoch 4 / batch 0\tPred loss: 1.0126"
      ],
      "id": "MeDpYJmEJIym"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loving-ghost"
      },
      "source": [
        "# Manually push data through network"
      ],
      "id": "loving-ghost"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guided-announcement"
      },
      "source": [
        "example_input = x_train[0]\n",
        "x = example_input\n",
        "x = x.view(1, x.size(0), -1).to(device)\n",
        "print(x.size())\n",
        "mu, logvar = model.encode(x)\n",
        "print(mu.shape, logvar.shape)\n",
        "\n",
        "z = model.reparameterize(mu, logvar)\n",
        "z.shape\n",
        "output = model.decode(z)\n",
        "print(\"decoded shape: \", output.shape)\n",
        "\n",
        "out, m, l, z = model.forward(x)\n",
        "vae_loss(out, x, m, l)"
      ],
      "id": "guided-announcement",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "renewable-soccer"
      },
      "source": [
        "model.prediction(z).shape  # TODO should we still have batch here?"
      ],
      "id": "renewable-soccer",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecological-cargo"
      },
      "source": [
        ""
      ],
      "id": "ecological-cargo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "polar-rouge"
      },
      "source": [
        ""
      ],
      "id": "polar-rouge",
      "execution_count": null,
      "outputs": []
    }
  ]
}